#!/usr/bin/env python3
"""
æ–‡æ¡£ URL è‡ªåŠ¨åŒ–æ£€æŸ¥è„šæœ¬

ç”¨äºå…¨é¢æ£€æŸ¥ open-lark é¡¹ç›®ä¸­çš„æ–‡æ¡£ URLï¼ŒåŒ…æ‹¬ï¼š
1. æ£€æŸ¥æ‰€æœ‰ API æ–¹æ³•çš„æ–‡æ¡£è¦†ç›–ç‡
2. éªŒè¯æ–‡æ¡£ URL çš„å¯è®¿é—®æ€§
3. è¯†åˆ«æ ¼å¼ä¸è§„èŒƒçš„æ–‡æ¡£ URL
4. ç”Ÿæˆè¯¦ç»†çš„æ£€æŸ¥æŠ¥å‘Š
"""

import os
import re
import sys
import time
import json
import asyncio
import aiohttp
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
from typing import Dict, List, Tuple, Optional, Set

class DocURLChecker:
    """æ–‡æ¡£ URL æ£€æŸ¥å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.src_dir = project_root / "src" / "service"
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files": 0,
                "total_methods": 0,
                "methods_with_docs": 0,
                "methods_without_docs": 0,
                "total_doc_urls": 0,
                "accessible_urls": 0,
                "inaccessible_urls": 0,
                "format_errors": 0
            },
            "files": {},
            "errors": [],
            "recommendations": []
        }

        # æ–‡æ¡£ URL æ¨¡å¼
        self.doc_patterns = [
            r'/// # APIæ–‡æ¡£\s*\n\s*///\s*(https://open\.feishu\.cn/document/[^\s\n]+)',
            r'/// <(https://open\.feishu\.cn/document/[^\s\n]+)>',
            r'https://open\.feishu\.cn/document/[^\s\n\)]+'
        ]

        # å¼‚æ­¥æ–¹æ³•æ¨¡å¼
        self.async_method_pattern = r'pub async fn\s+(\w+)'

        # ç”¨æˆ·ä»£ç†
        self.user_agent = "open-lark-doc-checker/1.0"

        # è¶…æ—¶è®¾ç½®
        self.timeout = 10

        # å¹¶å‘æ•°é™åˆ¶
        self.max_concurrent = 20

    def find_all_rust_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰ Rust æºæ–‡ä»¶"""
        if not self.src_dir.exists():
            print(f"âŒ æºç ç›®å½•ä¸å­˜åœ¨: {self.src_dir}")
            return []

        rust_files = list(self.src_dir.rglob("*.rs"))
        print(f"ğŸ“ æ‰¾åˆ° {len(rust_files)} ä¸ª Rust æ–‡ä»¶")
        return rust_files

    def analyze_file(self, file_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„æ–‡æ¡£çŠ¶æ€"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            error_msg = f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}"
            self.results["errors"].append(error_msg)
            return {"error": error_msg}

        # ç»Ÿè®¡å¼‚æ­¥æ–¹æ³•
        async_methods = re.findall(self.async_method_pattern, content)
        method_count = len(async_methods)

        # æŸ¥æ‰¾æ–‡æ¡£ URL
        doc_urls = []
        format_errors = []

        for pattern in self.doc_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    url = match[0] if match[0] else match[1] if len(match) > 1 else ""
                else:
                    url = match

                if url and url.startswith('https://open.feishu.cn/document/'):
                    doc_urls.append(url)
                elif url:
                    format_errors.append(f"æ— æ•ˆçš„æ–‡æ¡£ URL æ ¼å¼: {url}")

        # æ£€æŸ¥æ¯ä¸ªæ–¹æ³•æ˜¯å¦æœ‰æ–‡æ¡£
        methods_with_docs = set()

        # æŸ¥æ‰¾æ–¹æ³•ä¸æ–‡æ¡£çš„å…³è”
        for i, method in enumerate(async_methods):
            # åœ¨æ–¹æ³•å®šä¹‰å‰æŸ¥æ‰¾æ˜¯å¦æœ‰æ–‡æ¡£
            method_pattern = rf'pub async fn\s+{method}'
            method_matches = list(re.finditer(method_pattern, content))

            for match in method_matches:
                # æ£€æŸ¥æ–¹æ³•å®šä¹‰å‰çš„æ–‡æ¡£æ³¨é‡Š
                start_pos = max(0, match.start() - 500)  # å‘å‰æŸ¥æ‰¾500ä¸ªå­—ç¬¦
                text_before = content[start_pos:match.start()]

                if re.search(r'/// # APIæ–‡æ¡£', text_before) or re.search(r'/// <https://open\.feishu\.cn/document/', text_before):
                    methods_with_docs.add(method)

        # è®¡ç®—è¦†ç›–ç‡
        doc_count = len(methods_with_docs)
        coverage = (doc_count / method_count * 100) if method_count > 0 else 0

        return {
            "file_path": str(file_path.relative_to(self.project_root)),
            "total_methods": method_count,
            "methods_with_docs": doc_count,
            "methods_without_docs": method_count - doc_count,
            "doc_urls": doc_urls,
            "coverage": coverage,
            "format_errors": format_errors,
            "async_methods": async_methods
        }

    async def check_url_accessibility(self, session: aiohttp.ClientSession, url: str) -> Tuple[str, bool, str, int]:
        """æ£€æŸ¥å•ä¸ª URL çš„å¯è®¿é—®æ€§"""
        try:
            headers = {
                'User-Agent': self.user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }

            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=self.timeout)) as response:
                status_code = response.status
                is_accessible = 200 <= status_code < 400

                # å¯¹äºæˆåŠŸå“åº”ï¼Œæ£€æŸ¥å†…å®¹é•¿åº¦ä»¥ç¡®ä¿ä¸æ˜¯é”™è¯¯é¡µé¢
                if is_accessible:
                    content = await response.text()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å†…å®¹é¡µé¢
                    if len(content) < 1000 and ("404" in content or "Not Found" in content or "é¡µé¢ä¸å­˜åœ¨" in content):
                        is_accessible = False
                        return url, False, f"å†…å®¹è¿‡çŸ­ä¸”åŒ…å«é”™è¯¯ä¿¡æ¯ (status: {status_code})", status_code

                message = f"HTTP {status_code}" if is_accessible else f"HTTP {status_code} (ä¸å¯è®¿é—®)"
                return url, is_accessible, message, status_code

        except asyncio.TimeoutError:
            return url, False, "è¯·æ±‚è¶…æ—¶", 0
        except aiohttp.ClientError as e:
            return url, False, f"ç½‘ç»œé”™è¯¯: {str(e)}", 0
        except Exception as e:
            return url, False, f"æœªçŸ¥é”™è¯¯: {str(e)}", 0

    async def check_urls_batch(self, urls: List[str]) -> List[Tuple[str, bool, str, int]]:
        """æ‰¹é‡æ£€æŸ¥ URL å¯è®¿é—®æ€§"""
        if not urls:
            return []

        connector = aiohttp.TCPConnector(limit=self.max_concurrent, limit_per_host=5)
        timeout = aiohttp.ClientTimeout(total=self.timeout)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = [self.check_url_accessibility(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†å¼‚å¸¸
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    url = urls[i]
                    processed_results.append((url, False, f"æ£€æŸ¥å¼‚å¸¸: {str(result)}", 0))
                else:
                    processed_results.append(result)

            return processed_results

    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        summary = self.results["summary"]

        # æ–‡æ¡£è¦†ç›–ç‡å»ºè®®
        coverage_rate = (summary["methods_with_docs"] / summary["total_methods"] * 100) if summary["total_methods"] > 0 else 0
        if coverage_rate < 80:
            recommendations.append(f"ğŸ“ æ–‡æ¡£è¦†ç›–ç‡è¾ƒä½ ({coverage_rate:.1f}%)ï¼Œå»ºè®®ä¸ºæ‰€æœ‰ API æ–¹æ³•æ·»åŠ æ–‡æ¡£ URL")
        elif coverage_rate < 95:
            recommendations.append(f"ğŸ“ æ–‡æ¡£è¦†ç›–ç‡è‰¯å¥½ ({coverage_rate:.1f}%)ï¼Œå»ºè®®å®Œå–„å‰©ä½™æ–¹æ³•çš„æ–‡æ¡£")

        # URL å¯è®¿é—®æ€§å»ºè®®
        if summary["total_doc_urls"] > 0:
            accessibility_rate = (summary["accessible_urls"] / summary["total_doc_urls"] * 100)
            if accessibility_rate < 90:
                recommendations.append(f"ğŸ”— æ–‡æ¡£ URL å¯è®¿é—®æ€§è¾ƒä½ ({accessibility_rate:.1f}%)ï¼Œéœ€è¦ä¿®å¤å¤±æ•ˆé“¾æ¥")

        # æ ¼å¼é”™è¯¯å»ºè®®
        if summary["format_errors"] > 0:
            recommendations.append(f"ğŸ“ å‘ç° {summary['format_errors']} ä¸ªæ ¼å¼é”™è¯¯ï¼Œéœ€è¦æ ‡å‡†åŒ–æ–‡æ¡£ URL æ ¼å¼")

        # æŒ‰æ–‡ä»¶åˆ†æ
        low_coverage_files = []
        for file_path, file_info in self.results["files"].items():
            if file_info.get("coverage", 0) < 50 and file_info.get("total_methods", 0) > 0:
                low_coverage_files.append(file_path)

        if low_coverage_files:
            recommendations.append(f"ğŸ“ ä»¥ä¸‹æ–‡ä»¶æ–‡æ¡£è¦†ç›–ç‡è¾ƒä½ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨: {', '.join(low_coverage_files[:5])}")

        self.results["recommendations"] = recommendations

    def print_summary(self):
        """æ‰“å°æ£€æŸ¥æ‘˜è¦"""
        summary = self.results["summary"]

        print("\n" + "="*80)
        print("ğŸ“Š æ–‡æ¡£ URL æ£€æŸ¥æ‘˜è¦")
        print("="*80)

        print(f"ğŸ“ æ£€æŸ¥æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"ğŸ”§ æ€»å¼‚æ­¥æ–¹æ³•æ•°: {summary['total_methods']}")
        print(f"ğŸ“ æœ‰æ–‡æ¡£çš„æ–¹æ³•æ•°: {summary['methods_with_docs']}")
        print(f"âŒ æ— æ–‡æ¡£çš„æ–¹æ³•æ•°: {summary['methods_without_docs']}")

        if summary['total_methods'] > 0:
            coverage = (summary['methods_with_docs'] / summary['total_methods'] * 100)
            print(f"ğŸ“ˆ æ–‡æ¡£è¦†ç›–ç‡: {coverage:.1f}%")

        print(f"ğŸ”— æ€»æ–‡æ¡£ URL æ•°: {summary['total_doc_urls']}")
        print(f"âœ… å¯è®¿é—® URL æ•°: {summary['accessible_urls']}")
        print(f"âŒ ä¸å¯è®¿é—® URL æ•°: {summary['inaccessible_urls']}")

        if summary['total_doc_urls'] > 0:
            accessibility = (summary['accessible_urls'] / summary['total_doc_urls'] * 100)
            print(f"ğŸŒ URL å¯è®¿é—®ç‡: {accessibility:.1f}%")

        print(f"ğŸ“ æ ¼å¼é”™è¯¯æ•°: {summary['format_errors']}")

        # æ¨èå»ºè®®
        if self.results["recommendations"]:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(self.results["recommendations"], 1):
                print(f"   {i}. {rec}")

    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ è¯¦ç»†æ£€æŸ¥æŠ¥å‘Š")
        print("="*80)

        # æŒ‰è¦†ç›–ç‡æ’åºæ–‡ä»¶
        sorted_files = sorted(
            self.results["files"].items(),
            key=lambda x: x[1].get("coverage", 0)
        )

        for file_path, file_info in sorted_files:
            if "error" in file_info:
                print(f"\nâŒ {file_path}: {file_info['error']}")
                continue

            if file_info.get("total_methods", 0) == 0:
                continue

            coverage = file_info.get("coverage", 0)
            coverage_emoji = "ğŸŸ¢" if coverage >= 80 else "ğŸŸ¡" if coverage >= 50 else "ğŸ”´"

            print(f"\n{coverage_emoji} {file_path}")
            print(f"   æ–¹æ³•æ•°: {file_info['total_methods']}")
            print(f"   æœ‰æ–‡æ¡£: {file_info['methods_with_docs']}")
            print(f"   æ— æ–‡æ¡£: {file_info['methods_without_docs']}")
            print(f"   è¦†ç›–ç‡: {coverage:.1f}%")

            if file_info.get("format_errors"):
                print(f"   æ ¼å¼é”™è¯¯: {len(file_info['format_errors'])}")
                for error in file_info["format_errors"][:3]:
                    print(f"     - {error}")

            # æ˜¾ç¤ºç¼ºå¤±æ–‡æ¡£çš„æ–¹æ³•
            if file_info["methods_without_docs"] > 0:
                all_methods = set(file_info.get("async_methods", []))
                methods_with_docs = set()

                # é‡æ–°åˆ†ææ‰¾å‡ºæœ‰æ–‡æ¡£çš„æ–¹æ³•
                try:
                    full_path = self.project_root / file_path
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for method in all_methods:
                        method_pattern = rf'pub async fn\s+{method}'
                        if re.search(method_pattern, content):
                            # æ£€æŸ¥æ–¹æ³•å‰çš„æ–‡æ¡£
                            method_match = re.search(method_pattern, content)
                            if method_match:
                                start_pos = max(0, method_match.start() - 500)
                                text_before = content[start_pos:method_match.start()]
                                if re.search(r'/// # APIæ–‡æ¡£|/// <https://open\.feishu\.cn/document/', text_before):
                                    methods_with_docs.add(method)
                except:
                    pass

                missing_docs = all_methods - methods_with_docs
                if missing_docs:
                    print(f"   ç¼ºå¤±æ–‡æ¡£çš„æ–¹æ³•: {', '.join(list(missing_docs)[:5])}")
                    if len(missing_docs) > 5:
                        print(f"     ... è¿˜æœ‰ {len(missing_docs) - 5} ä¸ªæ–¹æ³•")

    def save_report(self, output_file: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"doc_check_report_{timestamp}.json"

        output_path = self.project_root / "scripts" / output_file

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    async def run(self, check_urls: bool = True, save_report: bool = False):
        """è¿è¡Œå®Œæ•´æ£€æŸ¥"""
        print("ğŸš€ å¼€å§‹æ–‡æ¡£ URL è‡ªåŠ¨åŒ–æ£€æŸ¥...")
        print("="*80)

        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
        files = self.find_all_rust_files()
        if not files:
            return

        print(f"ğŸ“Š åˆ†æ {len(files)} ä¸ªæ–‡ä»¶...")

        # åˆ†ææ‰€æœ‰æ–‡ä»¶
        for file_path in files:
            file_info = self.analyze_file(file_path)
            self.results["files"][str(file_path.relative_to(self.project_root))] = file_info

        # æ±‡æ€»ç»Ÿè®¡
        for file_info in self.results["files"].values():
            if "error" in file_info:
                continue

            self.results["summary"]["total_methods"] += file_info["total_methods"]
            self.results["summary"]["methods_with_docs"] += file_info["methods_with_docs"]
            self.results["summary"]["methods_without_docs"] += file_info["methods_without_docs"]
            self.results["summary"]["total_doc_urls"] += len(file_info["doc_urls"])
            self.results["summary"]["format_errors"] += len(file_info.get("format_errors", []))

        self.results["summary"]["total_files"] = len(files)

        # æ£€æŸ¥ URL å¯è®¿é—®æ€§
        if check_urls:
            all_urls = []
            for file_info in self.results["files"].values():
                all_urls.extend(file_info.get("doc_urls", []))

            # å»é‡
            unique_urls = list(set(all_urls))
            print(f"\nğŸŒ æ£€æŸ¥ {len(unique_urls)} ä¸ªå”¯ä¸€ URL çš„å¯è®¿é—®æ€§...")

            if unique_urls:
                url_results = await self.check_urls_batch(unique_urls)

                for url, is_accessible, message, status_code in url_results:
                    if is_accessible:
                        self.results["summary"]["accessible_urls"] += 1
                    else:
                        self.results["summary"]["inaccessible_urls"] += 1
                        self.results["errors"].append(f"URL ä¸å¯è®¿é—®: {url} - {message}")

        # ç”Ÿæˆå»ºè®®
        self.generate_recommendations()

        # æ‰“å°ç»“æœ
        self.print_summary()
        self.print_detailed_report()

        # ä¿å­˜æŠ¥å‘Š
        if save_report:
            self.save_report()

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ–‡æ¡£ URL è‡ªåŠ¨åŒ–æ£€æŸ¥å·¥å…·")
    parser.add_argument("--no-url-check", action="store_true", help="è·³è¿‡ URL å¯è®¿é—®æ€§æ£€æŸ¥")
    parser.add_argument("--save-report", action="store_true", help="ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶")
    parser.add_argument("--project-root", type=str, help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")

    args = parser.parse_args()

    # ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
    if args.project_root:
        project_root = Path(args.project_root)
    else:
        # è„šæœ¬åœ¨ scripts ç›®å½•ä¸‹ï¼Œé¡¹ç›®æ ¹ç›®å½•æ˜¯ä¸Šçº§ç›®å½•
        project_root = Path(__file__).parent.parent

    if not project_root.exists():
        print(f"âŒ é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {project_root}")
        sys.exit(1)

    # åˆ›å»ºæ£€æŸ¥å™¨å¹¶è¿è¡Œ
    checker = DocURLChecker(project_root)

    try:
        asyncio.run(checker.run(check_urls=not args.no_url_check, save_report=args.save_report))
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ£€æŸ¥è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()