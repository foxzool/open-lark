#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤æ‰€æœ‰å‰©ä½™çš„éæ ‡å‡†æ ¼å¼æ–‡æ¡£URL

è‡ªåŠ¨å¤„ç†æ‰€æœ‰ä¸ä½¿ç”¨æ ‡å‡†æ ¼å¼çš„URLï¼Œå°†å…¶æ›¿æ¢ä¸ºå®˜æ–¹APIæ•°æ®ä¸­çš„æ­£ç¡®URL
"""

import csv
import re
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json

def load_official_apis():
    """åŠ è½½å®˜æ–¹APIæ•°æ®"""
    official_apis = {}

    csv_path = Path(__file__).parent.parent / "reports" / "official_apis_by_module.csv"
    if not csv_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å®˜æ–¹APIæ•°æ®æ–‡ä»¶ {csv_path}")
        return {}

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            module = row['æ¨¡å—']
            method = row['HTTPæ–¹æ³•']
            endpoint = row['ç«¯ç‚¹è·¯å¾„']
            doc_url = row['æ–‡æ¡£URL']
            version = row['ç‰ˆæœ¬']
            name = row['APIåç§°']
            description = row['æè¿°']

            # åˆ›å»ºAPIæ ‡è¯†ç¬¦
            api_key = f"{module}:{method}:{endpoint}"

            if api_key not in official_apis:
                official_apis[api_key] = {
                    'module': module,
                    'method': method,
                    'endpoint': endpoint,
                    'doc_url': doc_url,
                    'version': version,
                    'name': name,
                    'description': description
                }

    print(f"âœ… æˆåŠŸåŠ è½½å®˜æ–¹APIæ•°æ®: {len(official_apis)} ä¸ªç«¯ç‚¹")
    return official_apis

def extract_url_pattern(url: str) -> Optional[str]:
    """ä»URLä¸­æå–APIè·¯å¾„æ¨¡å¼"""
    # åŒ¹é…å„ç§URLæ ¼å¼
    patterns = [
        r'open\.feishu\.cn/document/([^/]+(?:/v\d+)?)?/([^/]+)',
        r'document/([^/]+)/([^/]+)',
        r'([^/]+)/([^/]+)/([^/]+)$'  # module/version/endpoint æ ¼å¼
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(0)

    return None

def guess_api_info_from_url(url: str, file_path: Path) -> Dict:
    """ä»URLå’Œæ–‡ä»¶è·¯å¾„æ¨æ–­APIä¿¡æ¯"""
    # æå–æ–‡ä»¶è·¯å¾„ä¿¡æ¯
    path_parts = file_path.parts
    module = None
    service = None

    # å°è¯•ä»è·¯å¾„ä¸­æå–æ¨¡å—å
    for i, part in enumerate(path_parts):
        if part == "service" and i + 1 < len(path_parts):
            module = path_parts[i + 1]
            if i + 2 < len(path_parts):
                service = path_parts[i + 2]
            break

    # ä»URLä¸­æå–APIè·¯å¾„
    url_path = ""
    if "document/" in url:
        url_path = url.split("document/")[-1]
    elif "/document/" in url:
        url_path = url.split("/document/")[-1]

    return {
        'module': module,
        'service': service,
        'url_path': url_path,
        'original_url': url
    }

def find_best_match(api_info: Dict, official_apis: Dict) -> Optional[str]:
    """ä¸ºAPIä¿¡æ¯æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„å®˜æ–¹URL"""
    url_path = api_info['url_path']
    module = api_info.get('module', '')

    # åˆ†è§£URLè·¯å¾„ä¸ºå…³é”®å­—
    path_keywords = [kw for kw in url_path.split('/') if kw and len(kw) > 2]

    best_match = None
    best_score = 0

    for api_key, api_data in official_apis.items():
        score = 0

        # æ¨¡å—åŒ¹é…
        if api_data['module'].lower() == module.lower():
            score += 10

        # è·¯å¾„å…³é”®å­—åŒ¹é…
        for keyword in path_keywords:
            if keyword.lower() in api_data['endpoint'].lower():
                score += 5
            if keyword.lower() in api_data['doc_url'].lower():
                score += 3

        # APIåç§°åŒ¹é…
        for keyword in path_keywords:
            if keyword.lower() in api_data['name'].lower():
                score += 4
            if keyword.lower() in api_data['description'].lower():
                score += 2

        if score > best_score:
            best_score = score
            best_match = api_data['doc_url']

    # è®¾ç½®æœ€ä½åˆ†æ•°é˜ˆå€¼
    if best_score >= 3:
        return best_match

    return None

def fix_urls_in_file(file_path: Path, official_apis: Dict) -> int:
    """ä¿®å¤å•ä¸ªæ–‡ä»¶ä¸­çš„URL"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return 0

    # æŸ¥æ‰¾æ‰€æœ‰éœ€è¦ä¿®å¤çš„URLæ¨¡å¼
    url_patterns = [
        r'https://open\.feishu\.cn/document/[^/]+/[^/\s>]+(?:/v\d+)?/[^/\s>]+',
        r'https://open\.feishu\.cn/document/([^/]+)/([^/\s>]+)',
        r'https://open\.feishu\.cn/document/([^/]+)/([^/]+)/([^/\s>]+)'
    ]

    content_modified = content
    fixes_made = 0
    processed_urls = set()  # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªURL

    for pattern in url_patterns:
        matches = list(re.finditer(pattern, content))

        # æŒ‰URLé•¿åº¦é™åºå¤„ç†ï¼Œé¿å…çŸ­URLåŒ¹é…é•¿URLçš„ä¸€éƒ¨åˆ†
        matches.sort(key=lambda m: len(m.group(0)), reverse=True)

        for match in matches:
            old_url = match.group(0)

            if old_url in processed_urls:
                continue

            processed_urls.add(old_url)

            # è·³è¿‡å·²ç»æ˜¯æ ‡å‡†æ ¼å¼çš„URL
            if 'uAjLw4CM' in old_url or 'ukTMukTMukTM' in old_url:
                continue

            api_info = guess_api_info_from_url(old_url, file_path)
            correct_url = find_best_match(api_info, official_apis)

            if correct_url and old_url in content_modified:
                content_modified = content_modified.replace(old_url, correct_url)
                fixes_made += 1
                print(f"   âœ… ä¿®å¤: {old_url[:60]}... â†’ {correct_url[:60]}...")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°åŒ¹é…: {old_url[:60]}...")

    # å†™å›æ–‡ä»¶
    if fixes_made > 0:
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content_modified)
            print(f"   ğŸ“ {file_path.name}: {fixes_made} ä¸ªURLå·²ä¿®å¤")
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return 0

    return fixes_made

def find_files_with_non_standard_urls():
    """æŸ¥æ‰¾åŒ…å«éæ ‡å‡†URLçš„æ–‡ä»¶"""
    base_path = Path(__file__).parent.parent / "src" / "service"
    files_to_fix = []

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.rs') and not file.endswith('.bak'):
                file_path = Path(root) / file
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # æŸ¥æ‰¾åŒ…å«open.feishu.cn/documentä½†ä¸æ˜¯æ ‡å‡†æ ¼å¼çš„URL
                        if ('https://open.feishu.cn/document' in content and
                            'uAjLw4CM' not in content and
                            'server-docs' not in content):
                            files_to_fix.append(file_path)
                except Exception as e:
                    print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    return files_to_fix

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä¿®å¤æ‰€æœ‰å‰©ä½™çš„éæ ‡å‡†æ ¼å¼æ–‡æ¡£URL")
    print("=" * 60)

    # 1. åŠ è½½å®˜æ–¹APIæ•°æ®
    official_apis = load_official_apis()
    if not official_apis:
        return

    # 2. æŸ¥æ‰¾éœ€è¦ä¿®å¤çš„æ–‡ä»¶
    print(f"\nğŸ” æŸ¥æ‰¾åŒ…å«éæ ‡å‡†URLçš„æ–‡ä»¶...")
    files_to_fix = find_files_with_non_standard_urls()

    if not files_to_fix:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦ä¿®å¤çš„æ–‡ä»¶")
        return

    print(f"ğŸ“ æ‰¾åˆ° {len(files_to_fix)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")

    # 3. æ‰¹é‡å¤„ç†æ–‡ä»¶
    total_fixes = 0
    processed_files = 0

    for file_path in files_to_fix:
        print(f"\nğŸ”§ å¤„ç†æ–‡ä»¶: {file_path.relative_to(Path(__file__).parent.parent)}")
        fixes = fix_urls_in_file(file_path, official_apis)
        total_fixes += fixes
        if fixes > 0:
            processed_files += 1

    print(f"\nğŸ‰ æ‰¹é‡ä¿®å¤å®Œæˆï¼")
    print(f"ğŸ“ å¤„ç†æ–‡ä»¶æ•°: {processed_files}")
    print(f"ğŸ”§ ä¿®å¤URLæ•°: {total_fixes}")

    # 4. éªŒè¯ç»“æœ
    print(f"\nğŸ” éªŒè¯ä¿®å¤ç»“æœ...")
    try:
        result = os.popen('python3 scripts/doc_url_checker_simple.py --no-url-check | grep "æ ¼å¼é”™è¯¯" | head -1').read()
        if result.strip():
            print(f"   ğŸ“Š å½“å‰æ ¼å¼é”™è¯¯çŠ¶æ€: {result.strip()}")

        # è·å–è¯¦ç»†ç»Ÿè®¡
        stats = os.popen('python3 scripts/doc_url_checker_simple.py --no-url-check | tail -10').read()
        if "è¦†ç›–ç‡" in stats:
            for line in stats.split('\n'):
                if 'è¦†ç›–ç‡' in line:
                    print(f"   ğŸ“ˆ {line.strip()}")
    except:
        print(f"   âš ï¸  æ— æ³•è·å–éªŒè¯ä¿¡æ¯")

if __name__ == "__main__":
    main()