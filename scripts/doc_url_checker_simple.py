#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ–‡æ¡£ URL è‡ªåŠ¨åŒ–æ£€æŸ¥è„šæœ¬

ç”¨äºå…¨é¢æ£€æŸ¥ open-lark é¡¹ç›®ä¸­çš„æ–‡æ¡£ URLï¼Œä¸ä¾èµ–å¤–éƒ¨åº“ã€‚
"""

import os
import re
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from urllib.parse import urlparse

class SimpleDocURLChecker:
    """ç®€åŒ–ç‰ˆæ–‡æ¡£ URL æ£€æŸ¥å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.src_dir = project_root / "src" / "service"
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files": 0,
                "total_methods": 0,
                "methods_with_docs": 0,
                "methods_without_docs": 0,
                "total_doc_urls": 0,
                "accessible_urls": 0,
                "inaccessible_urls": 0,
                "format_errors": 0,
                "standard_format_urls": 0,
                "old_format_urls": 0
            },
            "files": {},
            "errors": [],
            "recommendations": [],
            "inaccessible_urls": []
        }

        # æ–‡æ¡£ URL æ¨¡å¼
        self.standard_format_pattern = r'/// # APIæ–‡æ¡£\s*\n\s*///\s*\n\s*///\s*(https://open\.feishu\.cn/document/[^\s\n]+)'
        self.old_format_pattern = r'/// <(https://open\.feishu\.cn/document/[^\s\n]+)>'
        self.generic_url_pattern = r'https://open\.feishu\.cn/document/[^\s\n\)]+'

        # å¼‚æ­¥æ–¹æ³•æ¨¡å¼
        self.async_method_pattern = r'pub async fn\s+(\w+)'

        # ç”¨æˆ·ä»£ç†
        self.user_agent = "open-lark-doc-checker/1.0"

        # è¶…æ—¶è®¾ç½®
        self.timeout = 10

    def find_all_rust_files(self) -> list:
        """æŸ¥æ‰¾æ‰€æœ‰ Rust æºæ–‡ä»¶"""
        if not self.src_dir.exists():
            print(f"âŒ æºç ç›®å½•ä¸å­˜åœ¨: {self.src_dir}")
            return []

        rust_files = list(self.src_dir.rglob("*.rs"))
        print(f"ğŸ“ æ‰¾åˆ° {len(rust_files)} ä¸ª Rust æ–‡ä»¶")
        return rust_files

    def analyze_file(self, file_path: Path) -> dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„æ–‡æ¡£çŠ¶æ€"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            error_msg = f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}"
            self.results["errors"].append(error_msg)
            return {"error": error_msg}

        # ç»Ÿè®¡å¼‚æ­¥æ–¹æ³•
        async_methods = re.findall(self.async_method_pattern, content)
        method_count = len(async_methods)

        # æŸ¥æ‰¾ä¸åŒæ ¼å¼çš„æ–‡æ¡£ URL
        standard_urls = re.findall(self.standard_format_pattern, content)
        old_format_urls = re.findall(self.old_format_pattern, content)

        # æŸ¥æ‰¾æ‰€æœ‰æ–‡æ¡£ URLï¼ˆé¿å…é‡å¤ï¼‰
        all_doc_urls = re.findall(self.generic_url_pattern, content)

        # å»é‡
        doc_urls = list(set(all_doc_urls))

        # æ ¼å¼é”™è¯¯æ£€æµ‹
        format_errors = []

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–éæ ‡å‡†æ ¼å¼çš„ URL
        all_urls_with_markers = re.findall(r'///\s*([^\n]*https://open\.feishu\.cn/document/[^\s\n]+)', content)
        for url_with_marker in all_urls_with_markers:
            if not url_with_marker.strip().startswith('/// # APIæ–‡æ¡£') and not url_with_marker.strip().startswith('/// <https'):
                url = re.search(r'https://open\.feishu\.cn/document/[^\s\n]+', url_with_marker)
                if url:
                    format_errors.append(f"éæ ‡å‡†æ ¼å¼: {url.group()}")

        # æ£€æŸ¥æ¯ä¸ªæ–¹æ³•æ˜¯å¦æœ‰æ–‡æ¡£
        methods_with_docs = set()

        # åˆ†ææ¯ä¸ªæ–¹æ³•çš„æ–‡æ¡£çŠ¶æ€
        for method in async_methods:
            method_pattern = rf'pub async fn\s+{method}'
            method_match = re.search(method_pattern, content)

            if method_match:
                # æ£€æŸ¥æ–¹æ³•å®šä¹‰å‰çš„æ–‡æ¡£æ³¨é‡Š
                start_pos = max(0, method_match.start() - 1000)  # å‘å‰æŸ¥æ‰¾1000ä¸ªå­—ç¬¦
                text_before = content[start_pos:method_match.start()]

                if re.search(r'/// # APIæ–‡æ¡£', text_before) or re.search(r'/// <https://open\.feishu\.cn/document/', text_before):
                    methods_with_docs.add(method)

        # è®¡ç®—è¦†ç›–ç‡
        doc_count = len(methods_with_docs)
        coverage = (doc_count / method_count * 100) if method_count > 0 else 0

        return {
            "file_path": str(file_path.relative_to(self.project_root)),
            "total_methods": method_count,
            "methods_with_docs": doc_count,
            "methods_without_docs": method_count - doc_count,
            "doc_urls": doc_urls,
            "standard_format_urls": len(standard_urls),
            "old_format_urls": len(old_format_urls),
            "coverage": coverage,
            "format_errors": format_errors,
            "async_methods": async_methods,
            "methods_with_docs_list": list(methods_with_docs)
        }

    def check_url_accessibility(self, url: str) -> tuple:
        """æ£€æŸ¥å•ä¸ª URL çš„å¯è®¿é—®æ€§"""
        try:
            headers = {
                'User-Agent': self.user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }

            req = Request(url, headers=headers)

            with urlopen(req, timeout=self.timeout) as response:
                status_code = response.getcode()
                is_accessible = 200 <= status_code < 400

                # æ£€æŸ¥å†…å®¹é•¿åº¦
                content = response.read(5000).decode('utf-8', errors='ignore')
                if len(content) < 1000 and ("404" in content or "Not Found" in content or "é¡µé¢ä¸å­˜åœ¨" in content or "æ‰¾ä¸åˆ°" in content):
                    is_accessible = False
                    message = f"å†…å®¹è¿‡çŸ­ä¸”åŒ…å«é”™è¯¯ä¿¡æ¯ (status: {status_code})"
                else:
                    message = f"HTTP {status_code}"

                return url, is_accessible, message, status_code

        except HTTPError as e:
            return url, False, f"HTTPé”™è¯¯ {e.code}", e.code
        except URLError as e:
            return url, False, f"URLé”™è¯¯: {str(e)}", 0
        except Exception as e:
            return url, False, f"æœªçŸ¥é”™è¯¯: {str(e)}", 0

    def check_urls_sample(self, urls: list, max_check: int = 20) -> list:
        """æ£€æŸ¥éƒ¨åˆ† URL çš„å¯è®¿é—®æ€§ï¼ˆé¿å…è¿‡å¤šè¯·æ±‚ï¼‰"""
        if not urls:
            return []

        # é™åˆ¶æ£€æŸ¥æ•°é‡ï¼Œé¿å…å‘é€è¿‡å¤šè¯·æ±‚
        urls_to_check = urls[:max_check]
        print(f"   æ£€æŸ¥å‰ {len(urls_to_check)} ä¸ª URL çš„å¯è®¿é—®æ€§...")

        results = []
        for i, url in enumerate(urls_to_check, 1):
            print(f"   æ£€æŸ¥ URL {i}/{len(urls_to_check)}: {url[:60]}...")
            result = self.check_url_accessibility(url)
            results.append(result)

            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)

        return results

    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        summary = self.results["summary"]

        # æ–‡æ¡£è¦†ç›–ç‡å»ºè®®
        if summary["total_methods"] > 0:
            coverage_rate = (summary["methods_with_docs"] / summary["total_methods"] * 100)
            if coverage_rate < 80:
                recommendations.append(f"ğŸ“ æ–‡æ¡£è¦†ç›–ç‡è¾ƒä½ ({coverage_rate:.1f}%)ï¼Œå»ºè®®ä¸ºæ‰€æœ‰ API æ–¹æ³•æ·»åŠ æ–‡æ¡£ URL")
            elif coverage_rate < 95:
                recommendations.append(f"ğŸ“ æ–‡æ¡£è¦†ç›–ç‡è‰¯å¥½ ({coverage_rate:.1f}%)ï¼Œå»ºè®®å®Œå–„å‰©ä½™æ–¹æ³•çš„æ–‡æ¡£")

        # æ ¼å¼æ ‡å‡†åŒ–å»ºè®®
        if summary["old_format_urls"] > 0:
            recommendations.append(f"ğŸ“ å‘ç° {summary['old_format_urls']} ä¸ªæ—§æ ¼å¼æ–‡æ¡£ URLï¼Œå»ºè®®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼")

        if summary["format_errors"] > 0:
            recommendations.append(f"ğŸ“ å‘ç° {summary['format_errors']} ä¸ªæ ¼å¼é”™è¯¯ï¼Œéœ€è¦æ ‡å‡†åŒ–æ–‡æ¡£ URL æ ¼å¼")

        # URL å¯è®¿é—®æ€§å»ºè®®
        total_checked_urls = summary["accessible_urls"] + summary["inaccessible_urls"]
        if total_checked_urls > 0:
            accessibility_rate = (summary["accessible_urls"] / total_checked_urls * 100)
            if accessibility_rate < 90:
                recommendations.append(f"ğŸ”— æ–‡æ¡£ URL å¯è®¿é—®æ€§è¾ƒä½ ({accessibility_rate:.1f}%)ï¼Œéœ€è¦ä¿®å¤å¤±æ•ˆé“¾æ¥")

        # æŒ‰æ–‡ä»¶åˆ†æ
        low_coverage_files = []
        for file_path, file_info in self.results["files"].items():
            if file_info.get("coverage", 0) < 50 and file_info.get("total_methods", 0) > 0:
                low_coverage_files.append(file_path)

        if low_coverage_files:
            recommendations.append(f"ğŸ“ ä»¥ä¸‹æ–‡ä»¶æ–‡æ¡£è¦†ç›–ç‡è¾ƒä½ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨: {', '.join(low_coverage_files[:5])}")

        self.results["recommendations"] = recommendations

    def print_summary(self):
        """æ‰“å°æ£€æŸ¥æ‘˜è¦"""
        summary = self.results["summary"]

        print("\n" + "="*80)
        print("ğŸ“Š æ–‡æ¡£ URL æ£€æŸ¥æ‘˜è¦")
        print("="*80)

        print(f"ğŸ“ æ£€æŸ¥æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"ğŸ”§ æ€»å¼‚æ­¥æ–¹æ³•æ•°: {summary['total_methods']}")
        print(f"ğŸ“ æœ‰æ–‡æ¡£çš„æ–¹æ³•æ•°: {summary['methods_with_docs']}")
        print(f"âŒ æ— æ–‡æ¡£çš„æ–¹æ³•æ•°: {summary['methods_without_docs']}")

        if summary['total_methods'] > 0:
            coverage = (summary['methods_with_docs'] / summary['total_methods'] * 100)
            print(f"ğŸ“ˆ æ–‡æ¡£è¦†ç›–ç‡: {coverage:.1f}%")

        print(f"ğŸ”— æ€»æ–‡æ¡£ URL æ•°: {summary['total_doc_urls']}")
        print(f"âœ¨ æ ‡å‡†æ ¼å¼ URL æ•°: {summary['standard_format_urls']}")
        print(f"ğŸ“œ æ—§æ ¼å¼ URL æ•°: {summary['old_format_urls']}")
        print(f"âœ… å¯è®¿é—® URL æ•°: {summary['accessible_urls']}")
        print(f"âŒ ä¸å¯è®¿é—® URL æ•°: {summary['inaccessible_urls']}")

        if summary["accessible_urls"] + summary["inaccessible_urls"] > 0:
            accessibility = (summary["accessible_urls"] / (summary["accessible_urls"] + summary["inaccessible_urls"]) * 100)
            print(f"ğŸŒ URL å¯è®¿é—®ç‡: {accessibility:.1f}%")

        print(f"ğŸ“ æ ¼å¼é”™è¯¯æ•°: {summary['format_errors']}")

        # æ¨èå»ºè®®
        if self.results["recommendations"]:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(self.results["recommendations"], 1):
                print(f"   {i}. {rec}")

    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ è¯¦ç»†æ£€æŸ¥æŠ¥å‘Š")
        print("="*80)

        # æŒ‰è¦†ç›–ç‡æ’åºæ–‡ä»¶
        sorted_files = sorted(
            self.results["files"].items(),
            key=lambda x: x[1].get("coverage", 0)
        )

        for file_path, file_info in sorted_files:
            if "error" in file_info:
                print(f"\nâŒ {file_path}: {file_info['error']}")
                continue

            if file_info.get("total_methods", 0) == 0:
                continue

            coverage = file_info.get("coverage", 0)
            coverage_emoji = "ğŸŸ¢" if coverage >= 80 else "ğŸŸ¡" if coverage >= 50 else "ğŸ”´"

            print(f"\n{coverage_emoji} {file_path}")
            print(f"   æ–¹æ³•æ•°: {file_info['total_methods']}")
            print(f"   æœ‰æ–‡æ¡£: {file_info['methods_with_docs']}")
            print(f"   æ— æ–‡æ¡£: {file_info['methods_without_docs']}")
            print(f"   è¦†ç›–ç‡: {coverage:.1f}%")
            print(f"   æ ‡å‡†æ ¼å¼: {file_info['standard_format_urls']}")
            print(f"   æ—§æ ¼å¼: {file_info['old_format_urls']}")

            if file_info.get("format_errors"):
                print(f"   æ ¼å¼é”™è¯¯: {len(file_info['format_errors'])}")
                for error in file_info["format_errors"][:3]:
                    print(f"     - {error}")

            # æ˜¾ç¤ºç¼ºå¤±æ–‡æ¡£çš„æ–¹æ³•
            if file_info["methods_without_docs"] > 0:
                all_methods = set(file_info.get("async_methods", []))
                methods_with_docs = set(file_info.get("methods_with_docs_list", []))
                missing_docs = all_methods - methods_with_docs

                if missing_docs:
                    print(f"   ç¼ºå¤±æ–‡æ¡£çš„æ–¹æ³•: {', '.join(list(missing_docs)[:5])}")
                    if len(missing_docs) > 5:
                        print(f"     ... è¿˜æœ‰ {len(missing_docs) - 5} ä¸ªæ–¹æ³•")

        # æ˜¾ç¤ºä¸å¯è®¿é—®çš„ URL
        if self.results["inaccessible_urls"]:
            print(f"\nâŒ ä¸å¯è®¿é—®çš„ URL:")
            for url, is_accessible, message, status_code in self.results["inaccessible_urls"]:
                print(f"   {url} - {message}")

    def save_report(self, output_file: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"doc_check_report_{timestamp}.json"

        output_path = self.project_root / "scripts" / output_file

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    def run(self, check_urls: bool = True, save_report: bool = False):
        """è¿è¡Œå®Œæ•´æ£€æŸ¥"""
        print("ğŸš€ å¼€å§‹æ–‡æ¡£ URL è‡ªåŠ¨åŒ–æ£€æŸ¥...")
        print("="*80)

        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
        files = self.find_all_rust_files()
        if not files:
            return

        print(f"ğŸ“Š åˆ†æ {len(files)} ä¸ªæ–‡ä»¶...")

        # åˆ†ææ‰€æœ‰æ–‡ä»¶
        for file_path in files:
            file_info = self.analyze_file(file_path)
            self.results["files"][str(file_path.relative_to(self.project_root))] = file_info

        # æ±‡æ€»ç»Ÿè®¡
        for file_info in self.results["files"].values():
            if "error" in file_info:
                continue

            self.results["summary"]["total_methods"] += file_info["total_methods"]
            self.results["summary"]["methods_with_docs"] += file_info["methods_with_docs"]
            self.results["summary"]["methods_without_docs"] += file_info["methods_without_docs"]
            self.results["summary"]["total_doc_urls"] += len(file_info["doc_urls"])
            self.results["summary"]["standard_format_urls"] += file_info["standard_format_urls"]
            self.results["summary"]["old_format_urls"] += file_info["old_format_urls"]
            self.results["summary"]["format_errors"] += len(file_info.get("format_errors", []))

        self.results["summary"]["total_files"] = len(files)

        # æ£€æŸ¥ URL å¯è®¿é—®æ€§
        if check_urls:
            all_urls = []
            for file_info in self.results["files"].values():
                all_urls.extend(file_info.get("doc_urls", []))

            # å»é‡
            unique_urls = list(set(all_urls))
            print(f"\nğŸŒ å‘ç° {len(unique_urls)} ä¸ªå”¯ä¸€æ–‡æ¡£ URL")

            if unique_urls:
                url_results = self.check_urls_sample(unique_urls, max_check=15)

                for url, is_accessible, message, status_code in url_results:
                    if is_accessible:
                        self.results["summary"]["accessible_urls"] += 1
                    else:
                        self.results["summary"]["inaccessible_urls"] += 1
                        self.results["inaccessible_urls"].append((url, is_accessible, message, status_code))

        # ç”Ÿæˆå»ºè®®
        self.generate_recommendations()

        # æ‰“å°ç»“æœ
        self.print_summary()
        self.print_detailed_report()

        # ä¿å­˜æŠ¥å‘Š
        if save_report:
            self.save_report()

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ç®€åŒ–ç‰ˆæ–‡æ¡£ URL è‡ªåŠ¨åŒ–æ£€æŸ¥å·¥å…·")
    parser.add_argument("--no-url-check", action="store_true", help="è·³è¿‡ URL å¯è®¿é—®æ€§æ£€æŸ¥")
    parser.add_argument("--save-report", action="store_true", help="ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶")
    parser.add_argument("--project-root", type=str, help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")

    args = parser.parse_args()

    # ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
    if args.project_root:
        project_root = Path(args.project_root)
    else:
        # è„šæœ¬åœ¨ scripts ç›®å½•ä¸‹ï¼Œé¡¹ç›®æ ¹ç›®å½•æ˜¯ä¸Šçº§ç›®å½•
        project_root = Path(__file__).parent.parent

    if not project_root.exists():
        print(f"âŒ é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {project_root}")
        sys.exit(1)

    # åˆ›å»ºæ£€æŸ¥å™¨å¹¶è¿è¡Œ
    checker = SimpleDocURLChecker(project_root)

    try:
        checker.run(check_urls=not args.no_url_check, save_report=args.save_report)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ£€æŸ¥è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()