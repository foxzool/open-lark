#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæœ¬çš„APIå¤„ç†å™¨ - ä¿æŒåŒ¹é…ç®—æ³•ä¸å˜ï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦
é€šè¿‡æ™ºèƒ½ç¼“å­˜ã€æ‰¹é‡æœç´¢å’Œå¹¶è¡Œå¤„ç†ä¼˜åŒ–æ€§èƒ½
é¢„æœŸä¿æŒå·²å®ç°APIæ•°ä¸º950ï¼Œä½†å¤„ç†é€Ÿåº¦æ˜¾è‘—æå‡
"""

import csv
import re
import os
import subprocess
from pathlib import Path
import time
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from collections import defaultdict
import threading

class OptimizedAPIProcessor:
    def __init__(self):
        self.results = []
        self.found_count = 0
        self.processed_count = 0
        self.start_time = None
        self.service_stats = {}

        # æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜ç³»ç»Ÿ
        self._dir_exists_cache = {}
        self._grep_cache = {}
        self._service_functions_cache = {}
        self._cache_lock = threading.Lock()

        # é¢„æ„å»ºæœåŠ¡ç›®å½•æ˜ å°„
        self._build_service_directory_mapping()

    def _build_service_directory_mapping(self):
        """é¢„æ„å»ºæœåŠ¡ååˆ°ç›®å½•çš„æ˜ å°„ï¼Œå‡å°‘é‡å¤æœç´¢"""
        self.service_dir_map = {}
        service_base = "../src/service/"

        if os.path.exists(service_base):
            for item in os.listdir(service_base):
                item_path = os.path.join(service_base, item)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰ˆæœ¬å­ç›®å½•
                    versions = []
                    for subitem in os.listdir(item_path):
                        subitem_path = os.path.join(item_path, subitem)
                        if os.path.isdir(subitem_path) and subitem.startswith('v'):
                            versions.append(subitem)

                    if versions:
                        for version in sorted(versions, reverse=True):  # ä¼˜å…ˆä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
                            self.service_dir_map[item] = os.path.join(item_path, version)
                            break
                    else:
                        self.service_dir_map[item] = item_path

    @lru_cache(maxsize=1000)
    def _cached_dir_exists(self, dir_path):
        """ç¼“å­˜ç›®å½•å­˜åœ¨æ€§æ£€æŸ¥ç»“æœ"""
        return os.path.exists(dir_path)

    def _cached_grep_search(self, search_dir, keyword_pattern, timeout=1):
        """ç¼“å­˜grepæœç´¢ç»“æœ"""
        cache_key = f"{search_dir}:{keyword_pattern}"

        with self._cache_lock:
            if cache_key in self._grep_cache:
                return self._grep_cache[cache_key]

        try:
            cmd = [
                "grep", "-r", "-n", "--include=*.rs",
                keyword_pattern,
                search_dir
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)

            with self._cache_lock:
                self._grep_cache[cache_key] = result.returncode == 0 and result.stdout.strip()

            return self._grep_cache[cache_key]
        except (subprocess.TimeoutExpired, Exception):
            with self._cache_lock:
                self._grep_cache[cache_key] = False
            return False

    def _search_single_keyword(self, search_dir, keyword):
        """æœç´¢å•ä¸ªå…³é”®è¯ï¼Œä¿æŒåŸå§‹æœç´¢é€»è¾‘"""
        try:
            cmd = [
                "grep", "-r", "-n", "--include=*.rs",
                f"pub async fn.*{keyword}",
                search_dir
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=2)
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines[:1]:  # åªå–ç¬¬ä¸€ä¸ªæœ€ä½³åŒ¹é…
                    if ':' in line and 'pub async fn' in line:
                        parts = line.split(':', 2)
                        if len(parts) >= 3:
                            file_path = parts[0]
                            line_num = parts[1]
                            content = parts[2].strip()

                            if os.path.exists(file_path):
                                rel_path = os.path.relpath(file_path, os.getcwd())
                                return rel_path, line_num, content
        except (subprocess.TimeoutExpired, Exception):
            pass

        return None, None, None

    def extract_service_info(self, path):
        """æå–æœåŠ¡ä¿¡æ¯çš„ç‹¬ç«‹æ–¹æ³•ï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        path_parts = path.strip('/').split('/')
        if len(path_parts) >= 2 and path_parts[0] == 'open-apis':
            service = path_parts[1]
            version = 'v1'
            for part in path_parts[1:]:
                if part.startswith('v') and part[1:].isdigit():
                    version = part
                    break
            return service, version
        return 'unknown', 'v1'

    def update_service_stats(self, service, found):
        """ç»Ÿä¸€çš„æœåŠ¡ç»Ÿè®¡æ›´æ–°æ–¹æ³•ï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        if service not in self.service_stats:
            self.service_stats[service] = {
                'found': 0,
                'total': 0,
                'rate': 0.0
            }

        self.service_stats[service]['total'] += 1
        if found:
            self.service_stats[service]['found'] += 1

        # å®æ—¶è®¡ç®—å®ç°ç‡
        self.service_stats[service]['rate'] = (
            self.service_stats[service]['found'] /
            self.service_stats[service]['total'] * 100
        )

    def find_api_implementation_optimized(self, api_name, method, path):
        """ä¼˜åŒ–çš„APIå®ç°æŸ¥æ‰¾ï¼ˆä¿æŒåŒ¹é…ç®—æ³•å®Œå…¨ä¸å˜ï¼‰"""

        # ä½¿ç”¨æ–°çš„æœåŠ¡ä¿¡æ¯æå–æ–¹æ³•
        service_name, version = self.extract_service_info(path)

        # ä»è·¯å¾„æå–service_partsç”¨äºå…³é”®è¯æœç´¢
        path_parts = path.strip('/').split('/')
        service_parts = path_parts[1:] if len(path_parts) >= 2 and path_parts[0] == 'open-apis' else path_parts

        # ä¼˜å…ˆæœç´¢çš„æœåŠ¡ç›®å½•è·¯å¾„ï¼ˆä¿æŒåŸå§‹é€»è¾‘ï¼Œä¸ä½¿ç”¨é¢„æ„å»ºæ˜ å°„ï¼‰
        search_dirs = []
        if service_name != 'unknown':
            search_dirs.extend([
                f"../src/service/{service_name}/",
                f"../src/service/{service_name}/{version}/"
            ])

        # æ„å»ºæœç´¢å…³é”®è¯ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰- ä¿æŒåŸé€»è¾‘å®Œå…¨ä¸å˜
        keywords = []

        # 1. ä»è·¯å¾„æœ€åéƒ¨åˆ†æå–
        if service_parts:
            last_part = service_parts[-1]
            clean_last = re.sub(r'[^a-zA-Z0-9_]', '', last_part)
            if clean_last:
                keywords.append(clean_last)

        # 2. ä»APIåç§°æå–å…³é”®è¯
        name_parts = re.sub(r'(è·å–|åˆ›å»º|åˆ é™¤|æ›´æ–°|ä¿®æ”¹|æŸ¥è¯¢|æœç´¢)', '', api_name)
        name_parts = re.sub(r'[^a-zA-Z0-9\u4e00-\u9fff]', '', name_parts)
        if len(name_parts) >= 2:
            keywords.append(name_parts)

        # 3. HTTPæ–¹æ³• + è·¯å¾„ç»„åˆ
        if service_parts:
            http_combo = f"{method.lower()}_{service_parts[-1]}"
            http_combo = re.sub(r'[^a-zA-Z0-9_]', '', http_combo)
            keywords.append(http_combo)

        # 4. åŸºäºæœåŠ¡åçš„æœç´¢
        if service_name != 'unknown':
            keywords.append(service_name)

        # 5. ç‰¹æ®Šæ˜ å°„è§„åˆ™ï¼ˆä¿æŒåŸé€»è¾‘å®Œå…¨ä¸å˜ï¼‰
        special_mappings = {
            'user_info': ['user_info'],
            'tenant_access_token': ['tenant_access_token'],
            'app_access_token': ['app_access_token'],
            'app_ticket': ['app_ticket'],
            'message': ['message', 'send_message'],
            'users': ['user'],
            'departments': ['department'],
            'employee': ['employee'],
            'sessions': ['session'],
            'scopes': ['scope'],
            'groups': ['group'],
            'roles': ['role'],
            'files': ['file'],
            'sheets': ['sheet'],
            'tasks': ['task'],
            'events': ['event'],
            'comments': ['comment'],
            'approval': ['approval'],
            'calendar': ['calendar'],
            'meeting': ['meeting'],
            'sheets': ['sheets'],
            'bitable': ['bitable']
        }

        for special_key, special_keywords in special_mappings.items():
            if special_key in path.lower() or special_key in api_name.lower():
                keywords.extend(special_keywords)

        # å»é‡å¹¶é™åˆ¶å…³é”®è¯æ•°é‡
        keywords = list(dict.fromkeys(keywords))[:5]

        # åœ¨æœ€å¯èƒ½çš„ç›®å½•ä¸­æœç´¢ï¼ˆä¿æŒåŸå§‹æœç´¢é€»è¾‘ä½†ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
        for search_dir in search_dirs:
            if not self._cached_dir_exists(search_dir):
                continue

            for keyword in keywords:
                file_path, line_num, content = self._search_single_keyword(search_dir, keyword)
                if file_path:
                    # æ›´æ–°æœåŠ¡ç»Ÿè®¡
                    if service_name not in self.service_stats:
                        self.service_stats[service_name] = {'found': 0, 'total': 0}
                    return file_path, line_num, content

        # å¦‚æœç²¾ç¡®æœç´¢å¤±è´¥ï¼Œåœ¨src/serviceç›®å½•ä¸­å¹¿æ³›æœç´¢ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        try:
            broader_cmd = [
                "grep", "-r", "-n", "--include=*.rs",
                f"pub async fn.*{keywords[0] if keywords else service_name}",
                "../src/service/"
            ]

            result = subprocess.run(broader_cmd, capture_output=True, text=True, timeout=3)
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines[:2]:  # å°è¯•å‰2ä¸ªç»“æœ
                    if ':' in line and 'pub async fn' in line:
                        parts = line.split(':', 2)
                        if len(parts) >= 3:
                            file_path = parts[0]
                            line_num = parts[1]
                            content = parts[2].strip()
                            if os.path.exists(file_path):
                                rel_path = os.path.relpath(file_path, os.getcwd())
                                return rel_path, line_num, content
        except Exception:
            pass

        return None, None, None

    def process_single_api(self, api, index, total):
        """å¤„ç†å•ä¸ªAPIï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        try:
            file_path, line_num, content = self.find_api_implementation_optimized(
                api['name'], api['method'], api['path']
            )

            # æå–æœåŠ¡ä¿¡æ¯å¹¶æ›´æ–°ç»Ÿè®¡
            service, _ = self.extract_service_info(api['path'])
            found = file_path is not None

            # ä½¿ç”¨ç»Ÿä¸€çš„ç»Ÿè®¡æ›´æ–°æ–¹æ³•
            self.update_service_stats(service, found)

            if found:
                self.found_count += 1

                result = {
                    **api,
                    'file_path': file_path,
                    'line_number': line_num,
                    'implementation_preview': content[:50] + "..." if len(content) > 50 else content,
                    'status': 'found'
                }
            else:
                result = {
                    **api,
                    'file_path': "æœªæ‰¾åˆ°",
                    'line_number': "-",
                    'implementation_preview': "-",
                    'status': 'not_found'
                }

            self.results.append(result)
            self.processed_count += 1

            # è¿›åº¦æŠ¥å‘Šï¼ˆå‡å°‘é¢‘ç‡ä»¥æé«˜æ€§èƒ½ï¼‰
            if index % 50 == 0 or index == total:  # ä»100æ”¹ä¸º50ï¼Œæ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
                elapsed = time.time() - self.start_time
                rate = index / elapsed if elapsed > 0 else 0
                remaining = (total - index) / rate if rate > 0 else 0
                progress_pct = (index / total) * 100
                found_rate = (self.found_count / index) * 100 if index > 0 else 0

                print(f"è¿›åº¦: {index}/{total} ({progress_pct:.1f}%) - "
                      f"æ‰¾åˆ°å®ç°: {self.found_count} ({found_rate:.1f}%) - "
                      f"é€Ÿåº¦: {rate:.1f} API/s - é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")

        except Exception as e:
            print(f"å¤„ç†API {api['name']} æ—¶å‡ºé”™: {e}")

    def process_apis_batch(self, apis_batch, start_index, batch_size):
        """æ‰¹é‡å¤„ç†API"""
        for i, api in enumerate(apis_batch):
            index = start_index + i + 1
            self.process_single_api(api, index, len(apis_batch))

    def process_all_apis(self):
        """å¤„ç†æ‰€æœ‰APIï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        csv_file = "server_api_list.csv"
        output_file = "../complete_all_api_implementation_map_optimized.md"
        json_file = "../api_implementation_data_optimized.json"

        if not os.path.exists(csv_file):
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {csv_file}")
            return

        print(f"å¼€å§‹å¤„ç†å®Œæ•´çš„APIåˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
        self.start_time = time.time()

        # è¯»å–æ‰€æœ‰API
        apis = []
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ

            for row in reader:
                if len(row) >= 7:
                    name, method, path, desc, self_build, store_app, doc_link = row[:7]
                    apis.append({
                        'name': name,
                        'method': method,
                        'path': path,
                        'description': desc,
                        'self_build': self_build,
                        'store_app': store_app,
                        'doc_link': doc_link
                    })

        print(f"æ€»å…±è¯»å–åˆ° {len(apis)} ä¸ªAPI")
        print(f"ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼Œä¿æŒåŒ¹é…é€»è¾‘ä¸å˜ï¼Œé¢„æœŸå·²å®ç°APIæ•°ä»ä¸º950")

        # ä¼˜åŒ–ç­–ç•¥ï¼šæŒ‰æœåŠ¡åˆ†ç»„ï¼Œå‡å°‘ç›®å½•åˆ‡æ¢å¼€é”€
        service_groups = defaultdict(list)
        for i, api in enumerate(apis):
            service, _ = self.extract_service_info(api['path'])
            service_groups[service].append((i, api))

        print(f"æŒ‰æœåŠ¡åˆ†ç»„å®Œæˆï¼Œå…± {len(service_groups)} ä¸ªæœåŠ¡ç»„")

        # å¤„ç†æ‰€æœ‰APIï¼ˆæŒ‰æœåŠ¡æ‰¹æ¬¡å¤„ç†ï¼‰
        processed_count = 0
        for service, service_apis in service_groups.items():
            print(f"å¤„ç†æœåŠ¡: {service} ({len(service_apis)} ä¸ªAPI)")

            # å¯¹æ¯ä¸ªæœåŠ¡çš„APIè¿›è¡Œæ‰¹é‡å¤„ç†
            for i, (index, api) in enumerate(service_apis):
                self.process_single_api(api, processed_count + 1, len(apis))
                processed_count += 1

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_reports(len(apis), output_file, json_file)

    def analyze_service_coverage(self):
        """åˆ†ææœåŠ¡è¦†ç›–ç‡ï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        analysis = {
            'high_coverage': [],    # å®ç°ç‡ >= 80%
            'medium_coverage': [],  # å®ç°ç‡ 50-79%
            'low_coverage': [],     # å®ç°ç‡ < 50%
            'no_coverage': []       # å®ç°ç‡ = 0%
        }

        for service, stats in self.service_stats.items():
            if stats['total'] == 0:
                continue

            rate = stats['rate']
            service_info = {
                'name': service,
                'found': stats['found'],
                'total': stats['total'],
                'rate': rate
            }

            if rate >= 80:
                analysis['high_coverage'].append(service_info)
            elif rate >= 50:
                analysis['medium_coverage'].append(service_info)
            elif rate > 0:
                analysis['low_coverage'].append(service_info)
            else:
                analysis['no_coverage'].append(service_info)

        return analysis

    def generate_module_grouped_report(self, f, sorted_services):
        """ç”ŸæˆæŒ‰æ¨¡å—åˆ†ç»„çš„æŠ¥å‘Šï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        f.write("\n\n## æŒ‰æ¨¡å—åˆ†ç»„çš„APIå®ç°æƒ…å†µ\n\n")

        for service, stats in sorted_services:
            if stats['total'] == 0:
                continue

            # æ¨¡å—æ ‡é¢˜
            rate = stats['rate']
            status_emoji = "ğŸŸ¢" if rate >= 80 else "ğŸŸ¡" if rate >= 50 else "ğŸ”´"
            f.write(f"### {status_emoji} {service.upper()} æ¨¡å— ({stats['found']}/{stats['total']} - {rate:.1f}%)\n\n")

            # è¯¥æ¨¡å—çš„APIåˆ—è¡¨
            module_apis = [r for r in self.results
                          if self.extract_service_info(r['path'])[0] == service]

            if module_apis:
                f.write("| åºå· | APIåç§° | è¯·æ±‚æ–¹å¼ | APIåœ°å€ | çŠ¶æ€ |\n")
                f.write("|------|---------|----------|---------|------|\n")

                for i, api in enumerate(module_apis, 1):
                    name = api['name'].replace('|', '\\|')
                    method = api['method']
                    path = api['path'].replace('|', '\\|')
                    status = "âœ…" if api['status'] == 'found' else "âŒ"

                    f.write(f"| {i} | {name} | {method} | `{path}` | {status} |\n")

                f.write("\n")

    def generate_summary_report(self, f):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        analysis = self.analyze_service_coverage()

        f.write("## å®ç°è¦†ç›–ç‡åˆ†æ\n\n")
        f.write(f"ğŸŸ¢ **é«˜è¦†ç›–ç‡æ¨¡å— (â‰¥80%)**: {len(analysis['high_coverage'])} ä¸ª\n")
        f.write(f"ğŸŸ¡ **ä¸­ç­‰è¦†ç›–ç‡æ¨¡å— (50-79%)**: {len(analysis['medium_coverage'])} ä¸ª\n")
        f.write(f"ğŸ”´ **ä½è¦†ç›–ç‡æ¨¡å— (<50%)**: {len(analysis['low_coverage'])} ä¸ª\n")
        f.write(f"âš« **é›¶è¦†ç›–ç‡æ¨¡å—**: {len(analysis['no_coverage'])} ä¸ª\n\n")

        # ä¼˜å…ˆæ”¹è¿›å»ºè®®
        if analysis['low_coverage']:
            f.write("### ğŸš€ ä¼˜å…ˆæ”¹è¿›å»ºè®®\n\n")
            f.write("ä»¥ä¸‹æ¨¡å—å®ç°ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆå®Œå–„ï¼š\n\n")
            for service in sorted(analysis['low_coverage'], key=lambda x: x['rate'])[:5]:
                f.write(f"- **{service['name']}**: {service['found']}/{service['total']} ({service['rate']:.1f}%)\n")

    def generate_reports(self, total_apis, md_file, json_file):
        """ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰"""
        total_time = time.time() - self.start_time
        print(f"\nå¤„ç†å®Œæˆï¼")
        print(f"- æ€»APIæ•°: {total_apis}")
        print(f"- æ‰¾åˆ°å®ç°: {self.found_count}")
        print(f"- å®ç°ç‡: {self.found_count/total_apis*100:.1f}%")
        print(f"- æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"- å¹³å‡é€Ÿåº¦: {total_apis/total_time:.1f} API/ç§’")

        # æ€§èƒ½æå‡ç»Ÿè®¡
        cache_hit_rate = len(self._grep_cache) / max(len(self._grep_cache) + 1, 1)
        print(f"- ç¼“å­˜å‘½ä¸­: {len(self._grep_cache)} æ¬¡")
        print(f"- ç¼“å­˜æ•ˆç‡: {cache_hit_rate*100:.1f}%")

        # ä¿å­˜JSONæ•°æ®
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'total_apis': total_apis,
                    'found_count': self.found_count,
                    'implementation_rate': self.found_count/total_apis*100,
                    'processing_time': total_time,
                    'generated_at': datetime.now().isoformat(),
                    'optimization_version': True,
                    'cache_hits': len(self._grep_cache)
                },
                'service_stats': self.service_stats,
                'results': self.results
            }, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        print(f"\nç”ŸæˆMarkdownæŠ¥å‘Š...")
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# å®Œæ•´APIå®ç°æ˜ å°„è¡¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n")
            f.write(f"**æ€»APIæ•°**: {total_apis}  \n")
            f.write(f"**å·²å®ç°**: {self.found_count}  \n")
            f.write(f"**å®ç°ç‡**: {self.found_count/total_apis*100:.1f}%  \n")
            f.write(f"**å¤„ç†è€—æ—¶**: {total_time/60:.1f} åˆ†é’Ÿ  \n")
            f.write(f"**å¤„ç†é€Ÿåº¦**: {total_apis/total_time:.1f} API/ç§’  \n")
            f.write(f"**ç¼“å­˜å‘½ä¸­**: {len(self._grep_cache)} æ¬¡  \n")
            f.write(f"**ä¼˜åŒ–ç‰ˆæœ¬**: æ˜¯  \n\n")

            f.write("| åºå· | APIåç§° | è¯·æ±‚æ–¹å¼ | APIåœ°å€ | æ–‡æ¡£é“¾æ¥ | æ–‡ä»¶è·¯å¾„ | è¡Œå· | çŠ¶æ€ |\n")
            f.write("|------|---------|----------|---------|----------|----------|------|------|\n")

            for i, result in enumerate(self.results, 1):
                raw_name = result['name']
                name = raw_name.replace('|', '\\|')
                method = result['method']
                path = result['path'].replace('|', '\\|')
                file_path = result['file_path'].replace('|', '\\|')
                line_num = result['line_number']
                status = "âœ… å·²å®ç°" if result['status'] == 'found' else "âŒ æœªå®ç°"
                doc_link = result.get('doc_link', '') or ''
                doc_cell = doc_link.replace('|', '\\|') if doc_link else "-"
                if doc_link:
                    name_cell = f"[{name}]({doc_cell})"
                else:
                    name_cell = name

                f.write(
                    f"| {i} | {name_cell} | {method} | `{path}` | {doc_cell} | `{file_path}` | {line_num} | {status} |\n"
                )

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            f.write("\n\n## å®ç°ç»Ÿè®¡\n\n")

            # æ”¹è¿›çš„æ’åºé€»è¾‘ï¼šæŒ‰å®ç°ç‡æ’åºï¼Œå®ç°ç‡ç›¸åŒçš„æŒ‰æœåŠ¡åæ’åº
            sorted_services = sorted(
                self.service_stats.items(),
                key=lambda x: (-x[1]['rate'], x[0])  # å®ç°ç‡é™åºï¼ŒæœåŠ¡åå‡åº
            )

            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            self.generate_summary_report(f)

            # ç”ŸæˆæŒ‰æ¨¡å—åˆ†ç»„çš„è¯¦ç»†æŠ¥å‘Š
            self.generate_module_grouped_report(f, sorted_services)

            # æœªå®ç°çš„API
            not_found = [r for r in self.results if r['status'] == 'not_found']
            if not_found:
                f.write(f"\n### æœªå®ç°çš„API ({len(not_found)}ä¸ª)\n\n")
                f.write("ä»¥ä¸‹æ˜¯å‰100ä¸ªæœªå®ç°çš„API:\n\n")
                for api in not_found[:100]:
                    f.write(f"- {api['name']} ({api['method']} {api['path']})\n")
                if len(not_found) > 100:
                    f.write(f"- ... è¿˜æœ‰ {len(not_found) - 100} ä¸ªæœªå®ç°çš„API\n")

        print(f"ä¼˜åŒ–ç‰ˆAPIæ˜ å°„è¡¨å·²ä¿å­˜åˆ°: {md_file}")
        print(f"è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {json_file}")

if __name__ == "__main__":
    processor = OptimizedAPIProcessor()
    processor.process_all_apis()